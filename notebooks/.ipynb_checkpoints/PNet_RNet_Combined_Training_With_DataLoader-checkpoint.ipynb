{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### Other dependencies ###\n",
    "from PIL import Image\n",
    "from dataloader.obj_detection import DataLoader\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "### Tensorflow dependencies ###\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.losses import MeanSquaredError, BinaryCrossentropy, CategoricalCrossentropy\n",
    "\n",
    "### Some constants ###\n",
    "input_dim = 12 # 48\n",
    "# weights_dir = 'road_signs_1'\n",
    "weights_dir = 'road_signs_w_dataloader'\n",
    "pnet_tensorboard_logdir = '../pnet_logs'\n",
    "rnet_tensorboard_logdir = '../rnet_logs'\n",
    "onet_tensorboard_logdir = '../onet_logs'\n",
    "\n",
    "pnet_weights = f'../weights/{weights_dir}/pnet.weights.hdf5'\n",
    "rnet_weights = f'../weights/{weights_dir}/rnet.weights.hdf5'\n",
    "onet_weights = f'../weights/{weights_dir}/onet.weights.hdf5'\n",
    "\n",
    "if(not os.path.exists(f'../weights/{weights_dir}')):\n",
    "    print('[INFO] Created weight directory ...')\n",
    "    os.mkdir(f'../weights/{weights_dir}')\n",
    "    \n",
    "if(os.path.exists(pnet_tensorboard_logdir)):\n",
    "    print('[INFO] Clearing P-Net log directory ... ')\n",
    "    shutil.rmtree(pnet_tensorboard_logdir)\n",
    "\n",
    "if(os.path.exists(rnet_tensorboard_logdir)):\n",
    "    print('[INFO] Clearing R-Net log directory ... ')\n",
    "    shutil.rmtree(rnet_tensorboard_logdir)\n",
    "\n",
    "if(os.path.exists(onet_tensorboard_logdir)):\n",
    "    print('[INFO] Clearing O-Net log directory ... ')\n",
    "    shutil.rmtree(onet_tensorboard_logdir)\n",
    "    \n",
    "epochs = 100 # 500\n",
    "batch_size = 16\n",
    "pnet_tensorboard = TensorBoard(log_dir=pnet_tensorboard_logdir)\n",
    "pnet_checkpoint = ModelCheckpoint(pnet_weights, save_weights_only=True)\n",
    "pnet_callbacks = [pnet_tensorboard, pnet_checkpoint]\n",
    "\n",
    "rnet_tensorboard = TensorBoard(log_dir=rnet_tensorboard_logdir)\n",
    "rnet_checkpoint = ModelCheckpoint(rnet_weights, save_weights_only=True)\n",
    "rnet_callbacks = [rnet_tensorboard, rnet_checkpoint]\n",
    "\n",
    "onet_tensorboard = TensorBoard(log_dir=onet_tensorboard_logdir)\n",
    "onet_checkpoint = ModelCheckpoint(onet_weights, save_weights_only=True)\n",
    "onet_early_stop1 = EarlyStopping(monitor='val_probability_loss', patience=15, verbose=1)\n",
    "onet_early_stop2 = EarlyStopping(monitor='val_bbox_regression_loss', patience=15, verbose=1)\n",
    "onet_callbacks = [onet_tensorboard, onet_checkpoint]\n",
    "\n",
    "train_dir = \"./road_signs/images\"\n",
    "test_dir = \"/home/minhhieu/Desktop/Hieu/datasets/GTSDB/TestIJCNN2013/TestIJCNN2013Download\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and explore dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating the train loader ###\n",
    "train_loader = DataLoader('/home/hieu/Downloads/road_signs_2/outputs/train', format_='darknet', \n",
    "                    color_space='rgb', img_size=input_dim, batch_size=64,\n",
    "                   crop_to_bounding_box=False)\n",
    "\n",
    "### Creating the test loader ###\n",
    "test_loader = DataLoader('/home/hieu/Downloads/road_signs_2/outputs/test', format_='darknet', \n",
    "                    color_space='rgb', img_size=input_dim, batch_size=64,\n",
    "                   crop_to_bounding_box=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = train_loader.get_train_dataset()\n",
    "val_dataset = train_loader.get_val_dataset()\n",
    "\n",
    "batch = next(iter(train_dataset))\n",
    "img, (bbox, label) = batch\n",
    "img = (img.numpy()[0] * 127.5 + 127.5).astype('uint8')\n",
    "H, W = img.shape[:2]\n",
    "x, y, w, h = (bbox.numpy()[0] * np.array([W, H, W, H])).astype('int')\n",
    "img = cv2.rectangle(img, (x, y), (x+w, y+h), (0,255,0), 1)\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement P-Net architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 1, 4) (1, 1, 1, 43)\n",
      "Model: \"P-Net\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "functional_1 (Functional)       (None, None, None, 1 330         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, None, None, 1 1456        functional_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_1 (PReLU)               (None, None, None, 1 16          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, None, None, 3 4640        p_re_lu_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_2 (PReLU)               (None, None, None, 3 32          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, None, None, 3 0           p_re_lu_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, None, None, 4 1419        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "probability (Softmax)           (None, None, None, 4 0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bbox_regression (Conv2D)        (None, None, None, 4 132         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 8,025\n",
      "Trainable params: 8,005\n",
      "Non-trainable params: 20\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def conv_block(in_filters, out_filters, kernel_size=3, batch_norm=False):\n",
    "    inputs = Input(shape=(None, None, in_filters))\n",
    "    p_layer = Conv2D(out_filters, kernel_size=kernel_size, strides=(1, 1), padding=\"valid\", kernel_regularizer=l2(2e-4))(inputs)\n",
    "    if(batch_norm) : p_layer = BatchNormalization()(p_layer)\n",
    "    p_layer = PReLU(shared_axes=[1, 2])(p_layer)\n",
    "        \n",
    "    p_layer = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"same\")(p_layer)\n",
    "    \n",
    "    block = Model(inputs = inputs, outputs=p_layer)\n",
    "    return block\n",
    "\n",
    "def build_pnet_model(input_shape=None, batch_norm=True, dropout=False, n_classes=2, activation='relu'):\n",
    "    if(input_shape is not None):\n",
    "        if(input_shape not in [12, 24, 48, 112, 224]):\n",
    "            raise Exception('Input shape must be in 12, 24, 48')\n",
    "    \n",
    "    inputs = Input(shape=(None, None, 3))\n",
    "    p_layer = conv_block(3, 10, kernel_size=3, batch_norm=batch_norm)(inputs)\n",
    "    \n",
    "    if(input_shape is not None):\n",
    "        if(input_shape >= 24):\n",
    "            p_layer = conv_block(10, 10, kernel_size=3, batch_norm=batch_norm)(p_layer)\n",
    "    \n",
    "    if(input_shape is not None):\n",
    "        if(input_shape >= 48):\n",
    "            p_layer = conv_block(10, 10, kernel_size=3, batch_norm=batch_norm)(p_layer)\n",
    "            \n",
    "    if(input_shape is not None):\n",
    "        if(input_shape >= 112):\n",
    "            p_layer = conv_block(10, 10, kernel_size=3, batch_norm=batch_norm)(p_layer)\n",
    "\n",
    "    p_layer = Conv2D(16, kernel_size=(3, 3), strides=(1, 1), padding=\"valid\", kernel_regularizer=l2(2e-4))(p_layer)\n",
    "    p_layer = PReLU(shared_axes=[1, 2])(p_layer)\n",
    "        \n",
    "    p_layer = Conv2D(32, kernel_size=(3, 3), strides=(1, 1), padding=\"valid\", kernel_regularizer=l2(2e-4))(p_layer)\n",
    "    p_layer = PReLU(shared_axes=[1, 2])(p_layer)\n",
    "    if(dropout) : p_layer = Dropout(0.5)(p_layer)\n",
    "\n",
    "    p_layer_out1 = Conv2D(n_classes, kernel_size=(1, 1), strides=(2, 2))(p_layer)\n",
    "    p_layer_out1 = Softmax(axis=3, name='probability')(p_layer_out1)\n",
    "    p_layer_out2 = Conv2D(4, kernel_size=(1, 1), strides=(2, 2), activation='sigmoid', name='bbox_regression')(p_layer)\n",
    "\n",
    "    p_net = Model(inputs, [p_layer_out1, p_layer_out2], name='P-Net')\n",
    "\n",
    "    return p_net\n",
    "\n",
    "\n",
    "### GIoU formula ###\n",
    "def GIoU(bboxes_1, bboxes_2, regularization=False):\n",
    "    # 1. calulate intersection over union\n",
    "    area_1 = (bboxes_1[..., 2] - bboxes_1[..., 0]) * (bboxes_1[..., 3] - bboxes_1[..., 1])\n",
    "    area_2 = (bboxes_2[..., 2] - bboxes_2[..., 0]) * (bboxes_2[..., 3] - bboxes_2[..., 1])\n",
    "    \n",
    "    intersection_wh = tf.minimum(bboxes_1[:, :, 2:], bboxes_2[:, :, 2:]) - tf.maximum(bboxes_1[:, :, :2], bboxes_2[:, :, :2])\n",
    "    intersection_wh = tf.maximum(intersection_wh, 0)\n",
    "    \n",
    "    intersection = intersection_wh[..., 0] * intersection_wh[..., 1]\n",
    "    union = (area_1 + area_2) - intersection\n",
    "    \n",
    "    ious = intersection / tf.maximum(union, 1e-10)\n",
    "\n",
    "    # 2. (C - (A U B))/C\n",
    "    C_wh = tf.maximum(bboxes_1[..., 2:], bboxes_2[..., 2:]) - tf.minimum(bboxes_1[..., :2], bboxes_2[..., :2])\n",
    "    C_wh = tf.maximum(C_wh, 0.0)\n",
    "    C = C_wh[..., 0] * C_wh[..., 1]\n",
    "    \n",
    "    # 3. Additional regularization - to preserve aspect ratio\n",
    "    lambda_ = 2e-4\n",
    "    w_reg = lambda_ * K.binary_crossentropy(bboxes_1[..., 2], bboxes_2[...,2])\n",
    "    h_reg = lambda_ * K.binary_crossentropy(bboxes_1[..., 3], bboxes_2[...,3])\n",
    "    \n",
    "    giou = ious - (C - union) / tf.maximum(C, 1e-10)\n",
    "    if(regularization):\n",
    "        giou += 0.5 * (w_reg + h_reg)\n",
    "        \n",
    "    return giou \n",
    "\n",
    "n_classes = train_loader.n_classes\n",
    "pnet = build_pnet_model(input_shape=input_dim, batch_norm=True, dropout=True,\n",
    "                        n_classes=n_classes)\n",
    "\n",
    "prob, bbox = pnet(tf.random.normal((1,12,12,3)))\n",
    "print(bbox.shape, prob.shape)\n",
    "print(pnet.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start training P-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses = {\n",
    "    'probability' : BinaryCrossentropy(from_logits=False),\n",
    "    'bbox_regression' : tfa.losses.GIoULoss() # MeanSquaredError(reduction=tf.keras.losses.Reduction.AUTO)\n",
    "}\n",
    "\n",
    "if(os.path.exists(pnet_weights)):\n",
    "    print('[INFO] Loading P-Net pretrained weights ...')\n",
    "    pnet.load_weights(pnet_weights)\n",
    "pnet.compile(optimizer=Adam(lr=0.00001, amsgrad=True),\n",
    "            loss=losses,\n",
    "            metrics={'probability':'accuracy'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1, 1, 43) (64, 1, 1, 43)\n",
      "(64, 1, 1, 4) (64, 1, 1, 4)\n",
      "(64, 1, 1, 43) (64, 1, 1, 43)\n",
      "(64, 1, 1, 4) (64, 1, 1, 4)\n",
      "[*] Batch #100, Epoch #1: Classification loss = 3.7756, BBox loss = 0.9998\n",
      "[*] Batch #200, Epoch #1: Classification loss = 3.7464, BBox loss = 0.9995\n",
      "[*] Batch #300, Epoch #1: Classification loss = 3.7884, BBox loss = 0.9993\n",
      "[*] Batch #400, Epoch #1: Classification loss = 3.7595, BBox loss = 1.0021\n",
      "[*] Batch #500, Epoch #1: Classification loss = 3.7278, BBox loss = 0.9998\n",
      "[*] Batch #600, Epoch #1: Classification loss = 3.7504, BBox loss = 0.9997\n",
      "Validating ... \n",
      "[*]    -> Val Classification loss = 3.754983425140381, Val BBox loss = 0.9999346733093262, Val accuracy = 0.0\n",
      "[*] Batch #100, Epoch #2: Classification loss = 3.7779, BBox loss = 0.9996\n",
      "[*] Batch #200, Epoch #2: Classification loss = 3.7888, BBox loss = 0.9993\n",
      "[*] Batch #300, Epoch #2: Classification loss = 3.7342, BBox loss = 0.9997\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-36be4463fe01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'[*]    -> Val Classification loss = {val_cls_loss}, Val BBox loss = {val_bbx_loss}, Val accuracy = {val_acc}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-36be4463fe01>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataset, val_dataset, steps_per_epoch, validation_steps, epochs)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mcls_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    413\u001b[0m     \"\"\"\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec, job_token)\u001b[0m\n\u001b[1;32m    694\u001b[0m           context.context().device_spec.device_type != \"CPU\"):\n\u001b[1;32m    695\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/cpu:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    720\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[1;32m    721\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_job_token\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m         \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         gen_experimental_dataset_ops.make_data_service_iterator(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3003\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3004\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3005\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   3006\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MakeIterator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3007\u001b[0m         tld.op_callbacks, dataset, iterator)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# history = pnet.fit(train_dataset, epochs=epochs, batch_size=batch_size, validation_data=val_dataset, callbacks=pnet_callbacks)\n",
    "\n",
    "steps_per_epoch = train_loader.dataset_len\n",
    "validation_steps = train_loader.val_len\n",
    "bce  = CategoricalCrossentropy(from_logits=False) # BinaryCrossentropy(from_logits=False)\n",
    "giou = tfa.losses.GIoULoss()\n",
    "opt = Adam(lr=0.00001, amsgrad=True)\n",
    "accuracy = tf.keras.metrics.Accuracy()\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        img, (bbox, prob) = batch\n",
    "        bbox = tf.expand_dims(bbox, axis=1)\n",
    "        bbox = tf.expand_dims(bbox, axis=1)\n",
    "        prob = tf.expand_dims(prob, axis=1)\n",
    "        prob = tf.expand_dims(prob, axis=1)\n",
    "        prob = tf.one_hot(prob, depth=n_classes)\n",
    "        pr_prob, pr_bbox = model(img, training=True)\n",
    "        \n",
    "        # print(pr_prob.shape, prob.shape)\n",
    "        # print(pr_bbox.shape, bbox.shape)\n",
    "        cls_loss = bce(prob, pr_prob)\n",
    "        bbx_loss = giou(bbox, pr_bbox)\n",
    "        \n",
    "        loss = cls_loss + bbx_loss\n",
    "        \n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        opt.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "    return cls_loss, bbx_loss\n",
    "\n",
    "@tf.function\n",
    "def validation_step(model, batch):\n",
    "    img, (bbox, prob) = batch\n",
    "    bbox = tf.expand_dims(bbox, axis=1)\n",
    "    bbox = tf.expand_dims(bbox, axis=1)\n",
    "    prob = tf.expand_dims(prob, axis=1)\n",
    "    prob = tf.expand_dims(prob, axis=1)\n",
    "    prob = tf.one_hot(prob, depth=n_classes)\n",
    "    pr_prob, pr_bbox = model(img, training=False)\n",
    "    \n",
    "    bbx_loss = giou(bbox, pr_bbox)\n",
    "    cls_loss = bce(prob, pr_prob)\n",
    "    acc = accuracy(prob, pr_prob)\n",
    "    \n",
    "    return cls_loss, bbx_loss, acc\n",
    "        \n",
    "def train(model, dataset, val_dataset, steps_per_epoch=1000, validation_steps=100, epochs=100):\n",
    "    for i in range(epochs):\n",
    "        for j in range(steps_per_epoch):\n",
    "            batch = next(iter(dataset))\n",
    "            \n",
    "            cls_loss, bbox_loss = train_step(model, batch)\n",
    "            cls_loss = cls_loss.numpy()\n",
    "            bbox_loss = bbox_loss.numpy()\n",
    "            \n",
    "            if((j + 1) % 100 == 0):\n",
    "                print(f'[*] Batch #{j+1}, Epoch #{i+1}: Classification loss = {cls_loss:.4f}, BBox loss = {bbox_loss:.4f}')\n",
    "            \n",
    "        print('Validating ... ')\n",
    "        val_cls_losses = []\n",
    "        val_bbx_losses = []\n",
    "        val_accuracies = []\n",
    "        \n",
    "        for j in range(validation_steps):\n",
    "            batch = next(iter(val_dataset))\n",
    "            cls_loss, bbox_loss, acc = validation_step(model, batch)\n",
    "            \n",
    "            val_cls_losses.append(cls_loss.numpy())\n",
    "            val_bbx_losses.append(bbox_loss.numpy())\n",
    "            val_accuracies.append(acc.numpy())\n",
    "            \n",
    "        val_cls_loss = np.array(val_cls_losses).mean()\n",
    "        val_bbx_loss = np.array(val_bbx_losses).mean()\n",
    "        val_acc = np.array(val_accuracies).mean()\n",
    "        \n",
    "        print(f'[*]    -> Val Classification loss = {val_cls_loss}, Val BBox loss = {val_bbx_loss}, Val accuracy = {val_acc}')\n",
    "            \n",
    "train(pnet, train_dataset, val_dataset, steps_per_epoch=steps_per_epoch, validation_steps=validation_steps, epochs=40)\n",
    "    \n",
    "\n",
    "print('[INFO] Training halted, plotting training history ... ')\n",
    "\n",
    "# history = history.history\n",
    "# fig, ax = plt.subplots(1, 2, figsize=(20, 8))\n",
    "# ax[0].plot(history['probability_loss'], color='orange', label='Training')\n",
    "# ax[0].plot(history['val_probability_loss'], color='blue', label='Validation')\n",
    "\n",
    "# ax[1].plot(history['bbox_regression_loss'], color='orange', label='Training')\n",
    "# ax[1].plot(history['val_bbox_regression_loss'], color='blue', label='Validation')\n",
    "\n",
    "# ax[0].legend()\n",
    "# ax[0].set_title('Classification losses')\n",
    "\n",
    "# ax[1].legend()\n",
    "# ax[1].set_title('BBox regression losses')\n",
    "\n",
    "# plt.savefig('PNet_training_result.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start training R-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate raw data from firectory\n",
    "raw_dataset = load_raw_dataset(train_dir, 'gt.txt')\n",
    "neg_samples = generate_neg_samples(raw_dataset, frame_per_img=5, crop_size=(input_dim*2,input_dim*2))\n",
    "pos_samples = generate_pos_samples(raw_dataset, pad_range=(10, 30), frame_per_img=4, img_size=input_dim*2)\n",
    "pos_samples[:, 2] = 1.0\n",
    "# pos_samples[:,2] = LabelEncoder().fit_transform(pos_samples[:, 2])\n",
    "# pos_samples[:,2] += 1\n",
    "\n",
    "# Concatenate two groups and shuffle\n",
    "train_dataset = np.concatenate([pos_samples, neg_samples])\n",
    "np.random.shuffle(train_dataset)\n",
    "\n",
    "train_images = np.array([x[0] for x in train_dataset])\n",
    "train_bboxes = np.array([x[1] for x in train_dataset])\n",
    "train_labels = OneHotEncoder().fit_transform(train_dataset[:,2].reshape(-1, 1)).toarray()\n",
    "\n",
    "train_bboxes = train_bboxes.reshape(-1, 1, 1, 4)\n",
    "train_labels = train_labels.reshape(-1, 1, 1, train_labels.shape[1])\n",
    "\n",
    "train_images = ((train_images - 127.5) / 127.5).astype('float32')\n",
    "train_bboxes = train_bboxes.astype('float32')\n",
    "train_labels = train_labels.astype('float32')\n",
    "\n",
    "losses = {\n",
    "    'probability' : BinaryCrossentropy(from_logits=False),\n",
    "    'bbox_regression' : tfa.losses.GIoULoss() \n",
    "    # 'bbox_regression' : MeanSquaredError(reduction=tf.keras.losses.Reduction.AUTO)\n",
    "}\n",
    "\n",
    "y = {\n",
    "    'probability' : train_labels,\n",
    "    'bbox_regression' : train_bboxes\n",
    "}\n",
    "\n",
    "rnet = build_pnet_model(input_shape=input_dim*2, batch_norm=True, dropout=True, n_classes=n_classes)\n",
    "print(rnet.summary())\n",
    "\n",
    "if(os.path.exists(rnet_weights)):\n",
    "    print('[INFO] Loading R-Net pretrained weights ...')\n",
    "    rnet.load_weights(rnet_weights)\n",
    "\n",
    "rnet.compile(optimizer=Adam(lr=0.00001, amsgrad=True),\n",
    "            loss=losses,\n",
    "            metrics={'probability':'accuracy'})\n",
    "\n",
    "history = rnet.fit(train_images, y, epochs=epochs, batch_size=batch_size, callbacks=rnet_callbacks, validation_split=0.2)\n",
    "print('[INFO] Training halted, plotting training history ... ')\n",
    "\n",
    "history = history.history\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 8))\n",
    "ax[0].plot(history['probability_loss'], color='orange', label='Training')\n",
    "ax[0].plot(history['val_probability_loss'], color='blue', label='Validation')\n",
    "\"\"\n",
    "ax[1].plot(history['bbox_regression_loss'], color='orange', label='Training')\n",
    "ax[1].plot(history['val_bbox_regression_loss'], color='blue', label='Validation')\n",
    "\n",
    "ax[0].legend()\n",
    "ax[0].set_title('Classification losses')\n",
    "\n",
    "ax[1].legend()\n",
    "ax[1].set_title('BBox regression losses')\n",
    "\n",
    "plt.savefig('RNet_training_result.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training O-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate raw data from firectory\n",
    "raw_dataset = load_raw_dataset(train_dir, 'gt.txt')\n",
    "neg_samples = generate_neg_samples(raw_dataset, frame_per_img=4, crop_size=(input_dim*4,input_dim*4))\n",
    "pos_samples = generate_pos_samples(raw_dataset, pad_range=(10, 30), frame_per_img=4, img_size=input_dim*4)\n",
    "pos_samples[:, 2] = 1.0\n",
    "\n",
    "# Concatenate two groups and shuffle\n",
    "train_dataset = np.concatenate([pos_samples, neg_samples])\n",
    "np.random.shuffle(train_dataset)\n",
    "\n",
    "train_images = np.array([x[0] for x in train_dataset])\n",
    "train_bboxes = np.array([x[1] for x in train_dataset])\n",
    "train_labels = OneHotEncoder().fit_transform(train_dataset[:,2].reshape(-1, 1)).toarray()\n",
    "\n",
    "train_bboxes = train_bboxes.reshape(-1, 1, 1, 4)\n",
    "train_labels = train_labels.reshape(-1, 1, 1, train_labels.shape[1])\n",
    "\n",
    "train_images = ((train_images - 127.5) / 127.5).astype('float32')\n",
    "train_bboxes = train_bboxes.astype('float32')\n",
    "train_labels = train_labels.astype('float32')\n",
    "\n",
    "losses = {\n",
    "    'probability' : BinaryCrossentropy(from_logits=False),\n",
    "    'bbox_regression' : tfa.losses.GIoULoss() \n",
    "}\n",
    "\n",
    "y = {\n",
    "    'probability' : train_labels,\n",
    "    'bbox_regression' : train_bboxes\n",
    "}\n",
    "\n",
    "onet = build_pnet_model(input_shape=input_dim*4, batch_norm=True, dropout=True, n_classes=n_classes)\n",
    "print(onet.summary())\n",
    "\n",
    "if(os.path.exists(onet_weights)):\n",
    "    print('[INFO] Loading O-Net pretrained weights ...')\n",
    "    onet.load_weights(onet_weights)\n",
    "onet.compile(optimizer=Adam(lr=0.00001, amsgrad=True),\n",
    "            loss=losses,\n",
    "            metrics={'probability':'accuracy'})\n",
    "\n",
    "history = onet.fit(train_images, y, epochs=epochs, validation_split=0.2, batch_size=batch_size, callbacks=onet_callbacks)\n",
    "print('[INFO] Training halted, plotting training history ... ')\n",
    "\n",
    "history = history.history\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 8))\n",
    "ax[0].plot(history['probability_loss'], color='orange', label='Training')\n",
    "ax[0].plot(history['val_probability_loss'], color='blue', label='Validation')\n",
    "\"\"\n",
    "ax[1].plot(history['bbox_regression_loss'], color='orange', label='Training')\n",
    "ax[1].plot(history['val_bbox_regression_loss'], color='blue', label='Validation')\n",
    "\n",
    "ax[0].legend()\n",
    "ax[0].set_title('Classification losses')\n",
    "\n",
    "ax[1].legend()\n",
    "ax[1].set_title('BBox regression losses')\n",
    "\n",
    "plt.savefig('ONet_training_result.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test P-Net proposals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet = build_pnet_model(input_shape=input_dim, batch_norm=True, dropout=True, n_classes=n_classes)\n",
    "rnet = build_pnet_model(input_shape=input_dim*2, batch_norm=True, dropout=True, n_classes=n_classes)\n",
    "onet = build_pnet_model(input_shape=input_dim*4, batch_norm=True, dropout=True, n_classes=n_classes)\n",
    "\n",
    "\n",
    "if(os.path.exists(pnet_weights)):\n",
    "    print('[INFO] Loading weights for P-Net ... ')\n",
    "    pnet.load_weights(pnet_weights)\n",
    "\n",
    "if(os.path.exists(rnet_weights)):\n",
    "    print('[INFO] Loading weights for R-Net ... ')\n",
    "    rnet.load_weights(rnet_weights)\n",
    "\n",
    "if(os.path.exists(onet_weights)):\n",
    "    print('[INFO] Loading weights for o-Net ... ')\n",
    "    onet.load_weights(onet_weights)\n",
    "    \n",
    "def __nms(boxes, s, threshold, method):\n",
    "    \"\"\"\n",
    "        Non Maximum Suppression.\n",
    "\n",
    "        Params:\n",
    "            @param boxes: np array with bounding boxes.\n",
    "            @param threshold:\n",
    "            @param method: NMS method to apply. Available values ('Min', 'Union')\n",
    "        \n",
    "        Return:\n",
    "            pick : An array of indices selected.\n",
    "    \"\"\"\n",
    "    if boxes.size == 0:\n",
    "        return np.empty((0, 3))\n",
    "\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2] + x1\n",
    "    y2 = boxes[:, 3] + y1\n",
    "\n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    sorted_s = np.argsort(s)\n",
    "\n",
    "    pick = np.zeros_like(s, dtype=np.int16)\n",
    "    counter = 0\n",
    "    while sorted_s.size > 0:\n",
    "        i = sorted_s[-1]\n",
    "        pick[counter] = i\n",
    "        counter += 1\n",
    "        idx = sorted_s[0:-1]\n",
    "\n",
    "        xx1 = np.maximum(x1[i], x1[idx])\n",
    "        yy1 = np.maximum(y1[i], y1[idx])\n",
    "        xx2 = np.minimum(x2[i], x2[idx])\n",
    "        yy2 = np.minimum(y2[i], y2[idx])\n",
    "\n",
    "        w = np.maximum(0.0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0.0, yy2 - yy1 + 1)\n",
    "\n",
    "        inter = w * h\n",
    "\n",
    "        if method == 'Min':\n",
    "            o = inter / np.minimum(area[i], area[idx])\n",
    "        else:\n",
    "            o = inter / (area[i] + area[idx] - inter)\n",
    "\n",
    "        sorted_s = sorted_s[np.where(o <= threshold)]\n",
    "\n",
    "    pick = pick[0:counter]\n",
    "\n",
    "    return pick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_img = cv2.imread(os.path.join(train_dir, 'road155.png'))\n",
    "# raw_img = cv2.imread(os.path.join(train_dir, 'road198.png'))\n",
    "# raw_img = cv2.imread(os.path.join(train_dir, 'road202.png')) # --> True but quite abit of overlapping boxes\n",
    "# raw_img = cv2.imread(os.path.join(train_dir, 'road215.png'))\n",
    "# raw_img = cv2.imread(os.path.join(train_dir, 'road797.png')) # --> True\n",
    "# raw_img = cv2.imread(os.path.join(train_dir, 'road655.png')) # --> True\n",
    "# raw_img = cv2.imread(os.path.join(train_dir, 'road420.png')) # --> True\n",
    "# raw_img = cv2.imread(os.path.join(train_dir, 'road123.png')) # --> True\n",
    "# raw_img = cv2.imread(os.path.join(train_dir, 'road109.png')) # --> True\n",
    "# raw_img = cv2.imread(os.path.join(train_dir, 'road108.png')) # --> False, target too small\n",
    "# raw_img = cv2.imread(os.path.join(train_dir, 'road99.png')) # --> Very true\n",
    "# raw_img = cv2.imread(os.path.join(train_dir, 'road90.png')) # --> True\n",
    "# raw_img = cv2.imread(os.path.join(train_dir, 'road282.png')) # --> False, too blurry\n",
    "# raw_img = cv2.imread(os.path.join(train_dir, 'road282.png')) # --> False, too blurry\n",
    "# raw_img = cv2.imread(os.path.join(train_dir, 'road147.png')) # --> True\n",
    "# raw_img = cv2.imread(os.path.join(train_dir, 'road585.png'))\n",
    "# raw_img = cv2.imread(os.path.join(train_dir, 'road595.png'))\n",
    "raw_img = cv2.imread('test/test4.jpg')\n",
    "raw_img = cv2.cvtColor(raw_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def get_bboxes_pnet(raw_img, threshold=0.5, nms_threshold=0.5,\n",
    "                    scale_factor=2.0, min_img_size = 48, padding = 0.15, visualize=False):\n",
    "    '''\n",
    "        \n",
    "    '''\n",
    "    H, W = raw_img.shape[:2]\n",
    "    images = [raw_img]\n",
    "    current_h, current_w = raw_img.shape[:2]\n",
    "    \n",
    "    ### 1. Get image pyramid ###\n",
    "    while(current_h > min_img_size and current_w > min_img_size):\n",
    "        current_h = int(current_h / scale_factor)\n",
    "        current_w = int(current_w / scale_factor)\n",
    "\n",
    "        if(current_w < min_img_size or current_h < min_img_size) : break\n",
    "\n",
    "        image = cv2.resize(raw_img, (current_w, current_h))\n",
    "        images.append(image)\n",
    "\n",
    "    ### 2. Get bounding boxes from each image in the pyramid ###\n",
    "    boxes = []\n",
    "    conf_maps_viz = []\n",
    "    for i, image in enumerate(images):\n",
    "        if(i == 0): scale = 1\n",
    "        else : scale = scale_factor ** i\n",
    "\n",
    "        img = (image - 127.5) / 127.5\n",
    "        height, width = image.shape[:2]\n",
    "\n",
    "        predictions = pnet.predict(np.array([img]))\n",
    "        features_shape = predictions[1][0].shape[:2]\n",
    "\n",
    "        scale_w = width / features_shape[1]\n",
    "        scale_h = height / features_shape[0]\n",
    "\n",
    "        bboxes = predictions[1][0]\n",
    "        raw_bboxes = bboxes\n",
    "        confidence = predictions[0][0]\n",
    "\n",
    "        ### Getting confidence map ###\n",
    "        conf_map = confidence[:, :, 1]\n",
    "        conf_map[conf_map > threshold] = 1.0\n",
    "        conf_map[conf_map <= threshold] = 0\n",
    "        conf_map = (conf_map * 255).astype(np.uint8)\n",
    "        contours, hierarchy = cv2.findContours(conf_map, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        conf_maps_viz.append(conf_map)\n",
    "        \n",
    "        for contour in contours:\n",
    "            rect = cv2.boundingRect(contour)\n",
    "            x, y, w, h = (rect * np.array([W/conf_map.shape[1],H/conf_map.shape[0],W/conf_map.shape[1],H/conf_map.shape[0] ])).astype(int)\n",
    "\n",
    "            x -= min(int(padding * w), x)\n",
    "            y -= min(int(padding * h), y)\n",
    "            w += 2*int(padding * w)\n",
    "            h += 2*int(padding * h)\n",
    "            if(w * h < (W * H)/64): continue\n",
    "            boxes.append([x,y,w,h])\n",
    "    \n",
    "    ### Performing nms ###\n",
    "    final_boxes = []\n",
    "    pick = __nms(np.array(boxes), np.ones((len(boxes))), nms_threshold, 'Min')\n",
    "    for i in range(len(boxes)):\n",
    "        final_boxes.append(boxes[i])\n",
    "        \n",
    "    crops = []\n",
    "    if(visualize):\n",
    "        fig, ax = plt.subplots(1, len(conf_maps_viz) + 1, figsize=(5*(len(conf_maps_viz) + 1), 5))\n",
    "        for i in range(len(conf_maps_viz)):\n",
    "            current_size = (H // (scale_factor ** i),W // (scale_factor**i))\n",
    "            ax[i].imshow(conf_maps_viz[i])\n",
    "            ax[i].set_title(f'Stage {i+1} {current_size}')\n",
    "        \n",
    "    raw_img_copy = raw_img.copy()\n",
    "    for (x, y, w, h) in final_boxes:\n",
    "        cv2.rectangle(raw_img_copy, (x,y), (x+w, y+h), (0,0,255),2)\n",
    "        crops.append([(x,y,w,h), raw_img[y:y+h, x:x+w]])\n",
    "            \n",
    "    if(visualize):\n",
    "        plt.imshow(raw_img_copy)\n",
    "        plt.show()\n",
    "        \n",
    "    return final_boxes, crops\n",
    "        \n",
    "boxes, crops = get_bboxes_pnet(raw_img, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_img = cv2.imread('test/test9.jpg')\n",
    "raw_img = cv2.cvtColor(raw_img, cv2.COLOR_BGR2RGB)\n",
    "boxes, crops = get_bboxes_pnet(raw_img, visualize=True)\n",
    "\n",
    "raw_img_copy = raw_img.copy()\n",
    "boxes = []\n",
    "\n",
    "n_frames = 0\n",
    "start = time.time()\n",
    "for i in crops:\n",
    "    n_frames += 1\n",
    "    crop = i[1]\n",
    "    (x, y, w, h) = i[0]\n",
    "    H, W = crop.shape[:2]\n",
    "    img = cv2.resize(crop, (input_dim*4, input_dim*4))\n",
    "    img = (img - 127.5) / 127.5\n",
    "    \n",
    "    prediction = onet.predict(np.array([img]))\n",
    "    confidence = prediction[0][0][0][0]\n",
    "    \n",
    "    label = np.argmax(confidence)\n",
    "    confidence = confidence[np.argmax(confidence)]\n",
    "    \n",
    "    bbox = prediction[1][0][0][0]\n",
    "    if(confidence < 0.98 or label == 0):continue\n",
    "    \n",
    "    x_,y_,w,h = (bbox * np.array([W, H, W, H])).astype('int')\n",
    "    x+=x_\n",
    "    y+=y_\n",
    "    \n",
    "    boxes.append([x, y, w, h])\n",
    "end = time.time()\n",
    "print(f'FPS : {n_frames/(end - start)}')\n",
    "\n",
    "pick = __nms(np.array(boxes), np.ones((len(boxes))), 0.3, 'Max')\n",
    "for i in pick:\n",
    "    x, y, w, h = boxes[i]\n",
    "    raw_img_copy = cv2.rectangle(raw_img_copy, (x, y), (x+w, y+h), (0,255,0), 3)\n",
    "    raw_img_copy = cv2.putText(raw_img_copy, f'{confidence:.2f}', (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,0,0), 3)\n",
    "    \n",
    "plt.imshow(raw_img_copy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
