{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import tqdm\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### Other dependencies ###\n",
    "from PIL import Image\n",
    "from dataloader.obj_detection import DataLoader\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "### Tensorflow dependencies ###\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.losses import MeanSquaredError, BinaryCrossentropy, CategoricalCrossentropy\n",
    "\n",
    "### Some constants ###\n",
    "input_dim = 12 # 48\n",
    "# weights_dir = 'road_signs_1'\n",
    "weights_dir = 'road_signs_w_dataloader'\n",
    "pnet_tensorboard_logdir = '../pnet_logs'\n",
    "rnet_tensorboard_logdir = '../rnet_logs'\n",
    "onet_tensorboard_logdir = '../onet_logs'\n",
    "\n",
    "pnet_weights = f'../weights/{weights_dir}/pnet.weights.hdf5'\n",
    "rnet_weights = f'../weights/{weights_dir}/rnet.weights.hdf5'\n",
    "onet_weights = f'../weights/{weights_dir}/onet.weights.hdf5'\n",
    "\n",
    "if(not os.path.exists(f'../weights/{weights_dir}')):\n",
    "    print('[INFO] Created weight directory ...')\n",
    "    os.mkdir(f'../weights/{weights_dir}')\n",
    "    \n",
    "if(os.path.exists(pnet_tensorboard_logdir)):\n",
    "    print('[INFO] Clearing P-Net log directory ... ')\n",
    "    shutil.rmtree(pnet_tensorboard_logdir)\n",
    "\n",
    "if(os.path.exists(rnet_tensorboard_logdir)):\n",
    "    print('[INFO] Clearing R-Net log directory ... ')\n",
    "    shutil.rmtree(rnet_tensorboard_logdir)\n",
    "\n",
    "if(os.path.exists(onet_tensorboard_logdir)):\n",
    "    print('[INFO] Clearing O-Net log directory ... ')\n",
    "    shutil.rmtree(onet_tensorboard_logdir)\n",
    "    \n",
    "epochs = 100 # 500\n",
    "batch_size = 16\n",
    "pnet_tensorboard = TensorBoard(log_dir=pnet_tensorboard_logdir)\n",
    "pnet_checkpoint = ModelCheckpoint(pnet_weights, save_weights_only=True)\n",
    "pnet_callbacks = [pnet_tensorboard, pnet_checkpoint]\n",
    "\n",
    "rnet_tensorboard = TensorBoard(log_dir=rnet_tensorboard_logdir)\n",
    "rnet_checkpoint = ModelCheckpoint(rnet_weights, save_weights_only=True)\n",
    "rnet_callbacks = [rnet_tensorboard, rnet_checkpoint]\n",
    "\n",
    "onet_tensorboard = TensorBoard(log_dir=onet_tensorboard_logdir)\n",
    "onet_checkpoint = ModelCheckpoint(onet_weights, save_weights_only=True)\n",
    "onet_early_stop1 = EarlyStopping(monitor='val_probability_loss', patience=15, verbose=1)\n",
    "onet_early_stop2 = EarlyStopping(monitor='val_bbox_regression_loss', patience=15, verbose=1)\n",
    "onet_callbacks = [onet_tensorboard, onet_checkpoint]\n",
    "\n",
    "train_dir = \"/home/minhhieu/Desktop/Hieu/datasets/GTSRB/outputs/train\"\n",
    "test_dir = \"/home/minhhieu/Desktop/Hieu/datasets/GTSRB/outputs/test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and explore dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating the train loader ###\n",
    "train_loader = DataLoader(train_dir, format_='darknet', \n",
    "                    color_space='rgb', img_size=input_dim, batch_size=64,\n",
    "                   crop_to_bounding_box=False)\n",
    "\n",
    "### Creating the test loader ###\n",
    "test_loader = DataLoader(test_dir, format_='darknet', \n",
    "                    color_space='rgb', img_size=input_dim, batch_size=64,\n",
    "                   crop_to_bounding_box=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = train_loader.get_train_dataset()\n",
    "val_dataset = train_loader.get_val_dataset()\n",
    "\n",
    "batch = next(iter(train_dataset))\n",
    "img, (bbox, label) = batch\n",
    "img = (img.numpy()[0] * 127.5 + 127.5).astype('uint8')\n",
    "H, W = img.shape[:2]\n",
    "x, y, w, h = (bbox.numpy()[0] * np.array([W, H, W, H])).astype('int')\n",
    "img = cv2.rectangle(img, (x, y), (x+w, y+h), (0,255,0), 1)\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement P-Net architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 1, 4) (1, 1, 1, 43)\n",
      "Model: \"P-Net\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "functional_1 (Functional)       (None, None, None, 1 330         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, None, None, 1 1456        functional_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_1 (PReLU)               (None, None, None, 1 16          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, None, None, 3 4640        p_re_lu_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_2 (PReLU)               (None, None, None, 3 32          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, None, None, 3 0           p_re_lu_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, None, None, 4 1419        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "probability (Softmax)           (None, None, None, 4 0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bbox_regression (Conv2D)        (None, None, None, 4 132         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 8,025\n",
      "Trainable params: 8,005\n",
      "Non-trainable params: 20\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def conv_block(in_filters, out_filters, kernel_size=3, batch_norm=False):\n",
    "    inputs = Input(shape=(None, None, in_filters))\n",
    "    p_layer = Conv2D(out_filters, kernel_size=kernel_size, strides=(1, 1), padding=\"valid\", kernel_regularizer=l2(2e-4))(inputs)\n",
    "    if(batch_norm) : p_layer = BatchNormalization()(p_layer)\n",
    "    p_layer = PReLU(shared_axes=[1, 2])(p_layer)\n",
    "        \n",
    "    p_layer = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"same\")(p_layer)\n",
    "    \n",
    "    block = Model(inputs = inputs, outputs=p_layer)\n",
    "    return block\n",
    "\n",
    "def build_pnet_model(input_shape=None, batch_norm=True, dropout=False, n_classes=2, activation='relu'):\n",
    "    if(input_shape is not None):\n",
    "        if(input_shape not in [12, 24, 48, 112, 224]):\n",
    "            raise Exception('Input shape must be in 12, 24, 48')\n",
    "    \n",
    "    inputs = Input(shape=(None, None, 3))\n",
    "    p_layer = conv_block(3, 10, kernel_size=3, batch_norm=batch_norm)(inputs)\n",
    "    \n",
    "    if(input_shape is not None):\n",
    "        if(input_shape >= 24):\n",
    "            p_layer = conv_block(10, 10, kernel_size=3, batch_norm=batch_norm)(p_layer)\n",
    "    \n",
    "    if(input_shape is not None):\n",
    "        if(input_shape >= 48):\n",
    "            p_layer = conv_block(10, 10, kernel_size=3, batch_norm=batch_norm)(p_layer)\n",
    "            \n",
    "    if(input_shape is not None):\n",
    "        if(input_shape >= 112):\n",
    "            p_layer = conv_block(10, 10, kernel_size=3, batch_norm=batch_norm)(p_layer)\n",
    "\n",
    "    p_layer = Conv2D(16, kernel_size=(3, 3), strides=(1, 1), padding=\"valid\", kernel_regularizer=l2(2e-4))(p_layer)\n",
    "    p_layer = PReLU(shared_axes=[1, 2])(p_layer)\n",
    "        \n",
    "    p_layer = Conv2D(32, kernel_size=(3, 3), strides=(1, 1), padding=\"valid\", kernel_regularizer=l2(2e-4))(p_layer)\n",
    "    p_layer = PReLU(shared_axes=[1, 2])(p_layer)\n",
    "    if(dropout) : p_layer = Dropout(0.5)(p_layer)\n",
    "\n",
    "    p_layer_out1 = Conv2D(n_classes, kernel_size=(1, 1), strides=(2, 2))(p_layer)\n",
    "    p_layer_out1 = Softmax(axis=3, name='probability')(p_layer_out1)\n",
    "    p_layer_out2 = Conv2D(4, kernel_size=(1, 1), strides=(2, 2), activation='sigmoid', name='bbox_regression')(p_layer)\n",
    "\n",
    "    p_net = Model(inputs, [p_layer_out1, p_layer_out2], name='P-Net')\n",
    "\n",
    "    return p_net\n",
    "\n",
    "\n",
    "### GIoU formula ###\n",
    "def GIoU(bboxes_1, bboxes_2, regularization=False):\n",
    "    # 1. calulate intersection over union\n",
    "    area_1 = (bboxes_1[..., 2] - bboxes_1[..., 0]) * (bboxes_1[..., 3] - bboxes_1[..., 1])\n",
    "    area_2 = (bboxes_2[..., 2] - bboxes_2[..., 0]) * (bboxes_2[..., 3] - bboxes_2[..., 1])\n",
    "    \n",
    "    intersection_wh = tf.minimum(bboxes_1[:, :, 2:], bboxes_2[:, :, 2:]) - tf.maximum(bboxes_1[:, :, :2], bboxes_2[:, :, :2])\n",
    "    intersection_wh = tf.maximum(intersection_wh, 0)\n",
    "    \n",
    "    intersection = intersection_wh[..., 0] * intersection_wh[..., 1]\n",
    "    union = (area_1 + area_2) - intersection\n",
    "    \n",
    "    ious = intersection / tf.maximum(union, 1e-10)\n",
    "\n",
    "    # 2. (C - (A U B))/C\n",
    "    C_wh = tf.maximum(bboxes_1[..., 2:], bboxes_2[..., 2:]) - tf.minimum(bboxes_1[..., :2], bboxes_2[..., :2])\n",
    "    C_wh = tf.maximum(C_wh, 0.0)\n",
    "    C = C_wh[..., 0] * C_wh[..., 1]\n",
    "    \n",
    "    # 3. Additional regularization - to preserve aspect ratio\n",
    "    lambda_ = 2e-4\n",
    "    w_reg = lambda_ * K.binary_crossentropy(bboxes_1[..., 2], bboxes_2[...,2])\n",
    "    h_reg = lambda_ * K.binary_crossentropy(bboxes_1[..., 3], bboxes_2[...,3])\n",
    "    \n",
    "    giou = ious - (C - union) / tf.maximum(C, 1e-10)\n",
    "    if(regularization):\n",
    "        giou += 0.5 * (w_reg + h_reg)\n",
    "        \n",
    "    return giou \n",
    "\n",
    "n_classes = train_loader.n_classes\n",
    "pnet = build_pnet_model(input_shape=input_dim, batch_norm=True, dropout=True,\n",
    "                        n_classes=n_classes)\n",
    "\n",
    "prob, bbox = pnet(tf.random.normal((1,12,12,3)))\n",
    "print(bbox.shape, prob.shape)\n",
    "print(pnet.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start training P-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses = {\n",
    "    'probability' : BinaryCrossentropy(from_logits=False),\n",
    "    'bbox_regression' : tfa.losses.GIoULoss() # MeanSquaredError(reduction=tf.keras.losses.Reduction.AUTO)\n",
    "}\n",
    "\n",
    "if(os.path.exists(pnet_weights)):\n",
    "    print('[INFO] Loading P-Net pretrained weights ...')\n",
    "    pnet.load_weights(pnet_weights)\n",
    "pnet.compile(optimizer=Adam(lr=0.00001, amsgrad=True),\n",
    "            loss=losses,\n",
    "            metrics={'probability':'accuracy'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/612 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 612/612 [01:04<00:00,  9.44it/s, batch_id=612, cls_loss=3.5173, bbox_loss=0.5023]\n",
      "  0%|          | 0/612 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 612/612 [01:00<00:00, 10.11it/s, batch_id=612, cls_loss=3.3899, bbox_loss=0.3133, accuracy=0.1249]\n",
      "  0%|          | 1/612 [00:00<01:01,  9.99it/s, batch_id=1, cls_loss=3.4781, bbox_loss=0.4707]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 612/612 [01:04<00:00,  9.52it/s, batch_id=612, cls_loss=3.1830, bbox_loss=0.4734]\n",
      "  0%|          | 2/612 [00:00<00:59, 10.22it/s, batch_id=2, cls_loss=3.1652, bbox_loss=0.2945, accuracy=0.1252]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 612/612 [01:01<00:00, 10.00it/s, batch_id=612, cls_loss=3.0221, bbox_loss=0.3308, accuracy=0.1990]\n",
      "  0%|          | 1/612 [00:00<01:03,  9.63it/s, batch_id=1, cls_loss=3.2848, bbox_loss=0.4438]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 612/612 [01:03<00:00,  9.59it/s, batch_id=612, cls_loss=2.8897, bbox_loss=0.3917]\n",
      "  0%|          | 2/612 [00:00<00:58, 10.51it/s, batch_id=2, cls_loss=2.8791, bbox_loss=0.2903, accuracy=0.1992]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 612/612 [01:00<00:00, 10.05it/s, batch_id=612, cls_loss=2.6651, bbox_loss=0.3074, accuracy=0.2531]\n",
      "  0%|          | 1/612 [00:00<01:04,  9.55it/s, batch_id=1, cls_loss=2.8979, bbox_loss=0.4255]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 612/612 [01:04<00:00,  9.52it/s, batch_id=612, cls_loss=2.5843, bbox_loss=0.4447]\n",
      "  0%|          | 2/612 [00:00<00:58, 10.43it/s, batch_id=2, cls_loss=2.4413, bbox_loss=0.3384, accuracy=0.2532]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 612/612 [01:01<00:00, 10.03it/s, batch_id=612, cls_loss=2.2447, bbox_loss=0.3497, accuracy=0.2966]\n",
      "  0%|          | 1/612 [00:00<01:03,  9.59it/s, batch_id=1, cls_loss=2.7976, bbox_loss=0.3707]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 612/612 [01:04<00:00,  9.52it/s, batch_id=612, cls_loss=2.4613, bbox_loss=0.4145]\n",
      "  0%|          | 2/612 [00:00<00:58, 10.41it/s, batch_id=2, cls_loss=2.0774, bbox_loss=0.3580, accuracy=0.2968]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 257/612 [00:25<00:35, 10.02it/s, batch_id=257, cls_loss=2.1609, bbox_loss=0.3150, accuracy=0.3131]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-480adec6abfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-480adec6abfb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataset, val_dataset, steps_per_epoch, validation_steps, epochs)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m                 \u001b[0mcls_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For Python 3 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    770\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 772\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    773\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;31m# and instead mimic ops placement in graphs: Operations on resource\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m         \u001b[0;31m# handles execute on the same device as where the resource is placed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m         ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[1;32m    756\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2602\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2603\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2604\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   2605\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"IteratorGetNext\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2606\u001b[0m         \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# history = pnet.fit(train_dataset, epochs=epochs, batch_size=batch_size, validation_data=val_dataset, callbacks=pnet_callbacks)\n",
    "\n",
    "steps_per_epoch = train_loader.dataset_len\n",
    "validation_steps = train_loader.val_len\n",
    "bce  = CategoricalCrossentropy(from_logits=False) # BinaryCrossentropy(from_logits=False)\n",
    "giou = tfa.losses.GIoULoss()\n",
    "opt = Adam(lr=0.0001, amsgrad=True)\n",
    "accuracy = tf.keras.metrics.Accuracy()\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        img, (bbox, prob) = batch\n",
    "        bbox = tf.expand_dims(bbox, axis=1)\n",
    "        bbox = tf.expand_dims(bbox, axis=1)\n",
    "        prob = tf.expand_dims(prob, axis=1)\n",
    "        prob = tf.expand_dims(prob, axis=1)\n",
    "        prob = tf.one_hot(prob, depth=n_classes)\n",
    "        pr_prob, pr_bbox = model(img, training=True)\n",
    "        \n",
    "        # print(pr_prob.shape, prob.shape)\n",
    "        # print(pr_bbox.shape, bbox.shape)\n",
    "        cls_loss = bce(prob, pr_prob)\n",
    "        bbx_loss = giou(bbox, pr_bbox)\n",
    "        \n",
    "        loss = cls_loss + bbx_loss\n",
    "        \n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        opt.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "    return cls_loss, bbx_loss\n",
    "\n",
    "@tf.function\n",
    "def validation_step(model, batch):\n",
    "    img, (bbox, prob) = batch\n",
    "    bbox = tf.expand_dims(bbox, axis=1)\n",
    "    bbox = tf.expand_dims(bbox, axis=1)\n",
    "    prob = tf.expand_dims(prob, axis=1)\n",
    "    prob = tf.expand_dims(prob, axis=1)\n",
    "    prob = tf.one_hot(prob, depth=n_classes)\n",
    "    pr_prob, pr_bbox = model(img, training=False)\n",
    "    \n",
    "    bbx_loss = giou(bbox, pr_bbox)\n",
    "    cls_loss = bce(prob, pr_prob)\n",
    "    acc = accuracy(tf.math.argmax(prob, axis=3), tf.math.argmax(pr_prob, axis=3))\n",
    "    \n",
    "    return cls_loss, bbx_loss, acc\n",
    "        \n",
    "def train(model, dataset, val_dataset, weights_file, steps_per_epoch=1000, validation_steps=100, epochs=100):\n",
    "    for i in range(epochs):\n",
    "        print(f'Epoch {i+1}/{epochs}')\n",
    "        with tqdm.tqdm(total=steps_per_epoch) as pbar:\n",
    "            for j in range(steps_per_epoch):\n",
    "                batch = next(iter(dataset))\n",
    "\n",
    "                cls_loss, bbox_loss = train_step(model, batch)\n",
    "                cls_loss = cls_loss.numpy()\n",
    "                bbox_loss = bbox_loss.numpy()\n",
    "\n",
    "                # if((j + 1) % 100 == 0):\n",
    "                #     print(f'[*] Batch #{j+1}, Epoch #{i+1}: Classification loss = {cls_loss:.4f}, BBox loss = {bbox_loss:.4f}')\n",
    "                \n",
    "                pbar.set_postfix({\n",
    "                    'batch_id' : j+1,\n",
    "                    'cls_loss': f'{cls_loss:.4f}',\n",
    "                    'bbox_loss' : f'{bbox_loss:.4f}'\n",
    "                })\n",
    "                pbar.update(1)\n",
    "                \n",
    "        print('Saving model weights ... ')\n",
    "        model.save_weights(weights_file)\n",
    "        print('Validating ... ')\n",
    "        with tqdm.tqdm(total=validation_steps) as pbar:\n",
    "            for j in range(validation_steps):\n",
    "                batch = next(iter(val_dataset))\n",
    "                cls_loss, bbox_loss, acc = validation_step(model, batch)\n",
    "\n",
    "                pbar.set_postfix({\n",
    "                    'batch_id' : j + 1,\n",
    "                    'cls_loss' : f'{cls_loss:.4f}',\n",
    "                    'bbox_loss' : f'{bbox_loss:.4f}',\n",
    "                    'accuracy' : f'{acc:.2f}'\n",
    "                })\n",
    "                pbar.update(1)\n",
    "    \n",
    "train(pnet, train_dataset, val_dataset, pnet_weights, steps_per_epoch=steps_per_epoch, validation_steps=validation_steps, epochs=40)\n",
    "    \n",
    "\n",
    "print('[INFO] Training halted, plotting training history ... ')\n",
    "\n",
    "# history = history.history\n",
    "# fig, ax = plt.subplots(1, 2, figsize=(20, 8))\n",
    "# ax[0].plot(history['probability_loss'], color='orange', label='Training')\n",
    "# ax[0].plot(history['val_probability_loss'], color='blue', label='Validation')\n",
    "\n",
    "# ax[1].plot(history['bbox_regression_loss'], color='orange', label='Training')\n",
    "# ax[1].plot(history['val_bbox_regression_loss'], color='blue', label='Validation')\n",
    "\n",
    "# ax[0].legend()\n",
    "# ax[0].set_title('Classification losses')\n",
    "\n",
    "# ax[1].legend()\n",
    "# ax[1].set_title('BBox regression losses')\n",
    "\n",
    "# plt.savefig('PNet_training_result.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start training R-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate raw data from firectory\n",
    "raw_dataset = load_raw_dataset(train_dir, 'gt.txt')\n",
    "neg_samples = generate_neg_samples(raw_dataset, frame_per_img=5, crop_size=(input_dim*2,input_dim*2))\n",
    "pos_samples = generate_pos_samples(raw_dataset, pad_range=(10, 30), frame_per_img=4, img_size=input_dim*2)\n",
    "pos_samples[:, 2] = 1.0\n",
    "# pos_samples[:,2] = LabelEncoder().fit_transform(pos_samples[:, 2])\n",
    "# pos_samples[:,2] += 1\n",
    "\n",
    "# Concatenate two groups and shuffle\n",
    "train_dataset = np.concatenate([pos_samples, neg_samples])\n",
    "np.random.shuffle(train_dataset)\n",
    "\n",
    "train_images = np.array([x[0] for x in train_dataset])\n",
    "train_bboxes = np.array([x[1] for x in train_dataset])\n",
    "train_labels = OneHotEncoder().fit_transform(train_dataset[:,2].reshape(-1, 1)).toarray()\n",
    "\n",
    "train_bboxes = train_bboxes.reshape(-1, 1, 1, 4)\n",
    "train_labels = train_labels.reshape(-1, 1, 1, train_labels.shape[1])\n",
    "\n",
    "train_images = ((train_images - 127.5) / 127.5).astype('float32')\n",
    "train_bboxes = train_bboxes.astype('float32')\n",
    "train_labels = train_labels.astype('float32')\n",
    "\n",
    "losses = {\n",
    "    'probability' : BinaryCrossentropy(from_logits=False),\n",
    "    'bbox_regression' : tfa.losses.GIoULoss() \n",
    "    # 'bbox_regression' : MeanSquaredError(reduction=tf.keras.losses.Reduction.AUTO)\n",
    "}\n",
    "\n",
    "y = {\n",
    "    'probability' : train_labels,\n",
    "    'bbox_regression' : train_bboxes\n",
    "}\n",
    "\n",
    "rnet = build_pnet_model(input_shape=input_dim*2, batch_norm=True, dropout=True, n_classes=n_classes)\n",
    "print(rnet.summary())\n",
    "\n",
    "if(os.path.exists(rnet_weights)):\n",
    "    print('[INFO] Loading R-Net pretrained weights ...')\n",
    "    rnet.load_weights(rnet_weights)\n",
    "\n",
    "rnet.compile(optimizer=Adam(lr=0.00001, amsgrad=True),\n",
    "            loss=losses,\n",
    "            metrics={'probability':'accuracy'})\n",
    "\n",
    "history = rnet.fit(train_images, y, epochs=epochs, batch_size=batch_size, callbacks=rnet_callbacks, validation_split=0.2)\n",
    "print('[INFO] Training halted, plotting training history ... ')\n",
    "\n",
    "history = history.history\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 8))\n",
    "ax[0].plot(history['probability_loss'], color='orange', label='Training')\n",
    "ax[0].plot(history['val_probability_loss'], color='blue', label='Validation')\n",
    "\"\"\n",
    "ax[1].plot(history['bbox_regression_loss'], color='orange', label='Training')\n",
    "ax[1].plot(history['val_bbox_regression_loss'], color='blue', label='Validation')\n",
    "\n",
    "ax[0].legend()\n",
    "ax[0].set_title('Classification losses')\n",
    "\n",
    "ax[1].legend()\n",
    "ax[1].set_title('BBox regression losses')\n",
    "\n",
    "plt.savefig('RNet_training_result.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training O-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate raw data from firectory\n",
    "raw_dataset = load_raw_dataset(train_dir, 'gt.txt')\n",
    "neg_samples = generate_neg_samples(raw_dataset, frame_per_img=4, crop_size=(input_dim*4,input_dim*4))\n",
    "pos_samples = generate_pos_samples(raw_dataset, pad_range=(10, 30), frame_per_img=4, img_size=input_dim*4)\n",
    "pos_samples[:, 2] = 1.0\n",
    "\n",
    "# Concatenate two groups and shuffle\n",
    "train_dataset = np.concatenate([pos_samples, neg_samples])\n",
    "np.random.shuffle(train_dataset)\n",
    "\n",
    "train_images = np.array([x[0] for x in train_dataset])\n",
    "train_bboxes = np.array([x[1] for x in train_dataset])\n",
    "train_labels = OneHotEncoder().fit_transform(train_dataset[:,2].reshape(-1, 1)).toarray()\n",
    "\n",
    "train_bboxes = train_bboxes.reshape(-1, 1, 1, 4)\n",
    "train_labels = train_labels.reshape(-1, 1, 1, train_labels.shape[1])\n",
    "\n",
    "train_images = ((train_images - 127.5) / 127.5).astype('float32')\n",
    "train_bboxes = train_bboxes.astype('float32')\n",
    "train_labels = train_labels.astype('float32')\n",
    "\n",
    "losses = {\n",
    "    'probability' : BinaryCrossentropy(from_logits=False),\n",
    "    'bbox_regression' : tfa.losses.GIoULoss() \n",
    "}\n",
    "\n",
    "y = {\n",
    "    'probability' : train_labels,\n",
    "    'bbox_regression' : train_bboxes\n",
    "}\n",
    "\n",
    "onet = build_pnet_model(input_shape=input_dim*4, batch_norm=True, dropout=True, n_classes=n_classes)\n",
    "print(onet.summary())\n",
    "\n",
    "if(os.path.exists(onet_weights)):\n",
    "    print('[INFO] Loading O-Net pretrained weights ...')\n",
    "    onet.load_weights(onet_weights)\n",
    "onet.compile(optimizer=Adam(lr=0.00001, amsgrad=True),\n",
    "            loss=losses,\n",
    "            metrics={'probability':'accuracy'})\n",
    "\n",
    "history = onet.fit(train_images, y, epochs=epochs, validation_split=0.2, batch_size=batch_size, callbacks=onet_callbacks)\n",
    "print('[INFO] Training halted, plotting training history ... ')\n",
    "\n",
    "history = history.history\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 8))\n",
    "ax[0].plot(history['probability_loss'], color='orange', label='Training')\n",
    "ax[0].plot(history['val_probability_loss'], color='blue', label='Validation')\n",
    "\"\"\n",
    "ax[1].plot(history['bbox_regression_loss'], color='orange', label='Training')\n",
    "ax[1].plot(history['val_bbox_regression_loss'], color='blue', label='Validation')\n",
    "\n",
    "ax[0].legend()\n",
    "ax[0].set_title('Classification losses')\n",
    "\n",
    "ax[1].legend()\n",
    "ax[1].set_title('BBox regression losses')\n",
    "\n",
    "plt.savefig('ONet_training_result.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test P-Net proposals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet = build_pnet_model(input_shape=input_dim, batch_norm=True, dropout=True, n_classes=n_classes)\n",
    "rnet = build_pnet_model(input_shape=input_dim*2, batch_norm=True, dropout=True, n_classes=n_classes)\n",
    "onet = build_pnet_model(input_shape=input_dim*4, batch_norm=True, dropout=True, n_classes=n_classes)\n",
    "\n",
    "\n",
    "if(os.path.exists(pnet_weights)):\n",
    "    print('[INFO] Loading weights for P-Net ... ')\n",
    "    pnet.load_weights(pnet_weights)\n",
    "\n",
    "if(os.path.exists(rnet_weights)):\n",
    "    print('[INFO] Loading weights for R-Net ... ')\n",
    "    rnet.load_weights(rnet_weights)\n",
    "\n",
    "if(os.path.exists(onet_weights)):\n",
    "    print('[INFO] Loading weights for o-Net ... ')\n",
    "    onet.load_weights(onet_weights)\n",
    "    \n",
    "def __nms(boxes, s, threshold, method):\n",
    "    \"\"\"\n",
    "        Non Maximum Suppression.\n",
    "\n",
    "        Params:\n",
    "            @param boxes: np array with bounding boxes.\n",
    "            @param threshold:\n",
    "            @param method: NMS method to apply. Available values ('Min', 'Union')\n",
    "        \n",
    "        Return:\n",
    "            pick : An array of indices selected.\n",
    "    \"\"\"\n",
    "    if boxes.size == 0:\n",
    "        return np.empty((0, 3))\n",
    "\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2] + x1\n",
    "    y2 = boxes[:, 3] + y1\n",
    "\n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    sorted_s = np.argsort(s)\n",
    "\n",
    "    pick = np.zeros_like(s, dtype=np.int16)\n",
    "    counter = 0\n",
    "    while sorted_s.size > 0:\n",
    "        i = sorted_s[-1]\n",
    "        pick[counter] = i\n",
    "        counter += 1\n",
    "        idx = sorted_s[0:-1]\n",
    "\n",
    "        xx1 = np.maximum(x1[i], x1[idx])\n",
    "        yy1 = np.maximum(y1[i], y1[idx])\n",
    "        xx2 = np.minimum(x2[i], x2[idx])\n",
    "        yy2 = np.minimum(y2[i], y2[idx])\n",
    "\n",
    "        w = np.maximum(0.0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0.0, yy2 - yy1 + 1)\n",
    "\n",
    "        inter = w * h\n",
    "\n",
    "        if method == 'Min':\n",
    "            o = inter / np.minimum(area[i], area[idx])\n",
    "        else:\n",
    "            o = inter / (area[i] + area[idx] - inter)\n",
    "\n",
    "        sorted_s = sorted_s[np.where(o <= threshold)]\n",
    "\n",
    "    pick = pick[0:counter]\n",
    "\n",
    "    return pick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_img = cv2.imread(os.path.join(train_dir, 'road155.png'))\n",
    "# raw_img = cv2.imread(os.path.join(train_dir, 'road198.png'))\n",
    "# raw_img = cv2.imread(os.path.join(train_dir, 'road202.png')) # --> True but quite abit of overlapping boxes\n",
    "# raw_img = cv2.imread(os.path.join(train_dir, 'road215.png'))\n",
    "# raw_img = cv2.imread(os.path.join(train_dir, 'road797.png')) # --> True\n",
    "# raw_img = cv2.imread(os.path.join(train_dir, 'road655.png')) # --> True\n",
    "# raw_img = cv2.imread(os.path.join(train_dir, 'road420.png')) # --> True\n",
    "# raw_img = cv2.imread(os.path.join(train_dir, 'road123.png')) # --> True\n",
    "# raw_img = cv2.imread(os.path.join(train_dir, 'road109.png')) # --> True\n",
    "# raw_img = cv2.imread(os.path.join(train_dir, 'road108.png')) # --> False, target too small\n",
    "# raw_img = cv2.imread(os.path.join(train_dir, 'road99.png')) # --> Very true\n",
    "# raw_img = cv2.imread(os.path.join(train_dir, 'road90.png')) # --> True\n",
    "# raw_img = cv2.imread(os.path.join(train_dir, 'road282.png')) # --> False, too blurry\n",
    "# raw_img = cv2.imread(os.path.join(train_dir, 'road282.png')) # --> False, too blurry\n",
    "# raw_img = cv2.imread(os.path.join(train_dir, 'road147.png')) # --> True\n",
    "# raw_img = cv2.imread(os.path.join(train_dir, 'road585.png'))\n",
    "# raw_img = cv2.imread(os.path.join(train_dir, 'road595.png'))\n",
    "raw_img = cv2.imread('test/test4.jpg')\n",
    "raw_img = cv2.cvtColor(raw_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def get_bboxes_pnet(raw_img, threshold=0.5, nms_threshold=0.5,\n",
    "                    scale_factor=2.0, min_img_size = 48, padding = 0.15, visualize=False):\n",
    "    '''\n",
    "        \n",
    "    '''\n",
    "    H, W = raw_img.shape[:2]\n",
    "    images = [raw_img]\n",
    "    current_h, current_w = raw_img.shape[:2]\n",
    "    \n",
    "    ### 1. Get image pyramid ###\n",
    "    while(current_h > min_img_size and current_w > min_img_size):\n",
    "        current_h = int(current_h / scale_factor)\n",
    "        current_w = int(current_w / scale_factor)\n",
    "\n",
    "        if(current_w < min_img_size or current_h < min_img_size) : break\n",
    "\n",
    "        image = cv2.resize(raw_img, (current_w, current_h))\n",
    "        images.append(image)\n",
    "\n",
    "    ### 2. Get bounding boxes from each image in the pyramid ###\n",
    "    boxes = []\n",
    "    conf_maps_viz = []\n",
    "    for i, image in enumerate(images):\n",
    "        if(i == 0): scale = 1\n",
    "        else : scale = scale_factor ** i\n",
    "\n",
    "        img = (image - 127.5) / 127.5\n",
    "        height, width = image.shape[:2]\n",
    "\n",
    "        predictions = pnet.predict(np.array([img]))\n",
    "        features_shape = predictions[1][0].shape[:2]\n",
    "\n",
    "        scale_w = width / features_shape[1]\n",
    "        scale_h = height / features_shape[0]\n",
    "\n",
    "        bboxes = predictions[1][0]\n",
    "        raw_bboxes = bboxes\n",
    "        confidence = predictions[0][0]\n",
    "\n",
    "        ### Getting confidence map ###\n",
    "        conf_map = confidence[:, :, 1]\n",
    "        conf_map[conf_map > threshold] = 1.0\n",
    "        conf_map[conf_map <= threshold] = 0\n",
    "        conf_map = (conf_map * 255).astype(np.uint8)\n",
    "        contours, hierarchy = cv2.findContours(conf_map, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        conf_maps_viz.append(conf_map)\n",
    "        \n",
    "        for contour in contours:\n",
    "            rect = cv2.boundingRect(contour)\n",
    "            x, y, w, h = (rect * np.array([W/conf_map.shape[1],H/conf_map.shape[0],W/conf_map.shape[1],H/conf_map.shape[0] ])).astype(int)\n",
    "\n",
    "            x -= min(int(padding * w), x)\n",
    "            y -= min(int(padding * h), y)\n",
    "            w += 2*int(padding * w)\n",
    "            h += 2*int(padding * h)\n",
    "            if(w * h < (W * H)/64): continue\n",
    "            boxes.append([x,y,w,h])\n",
    "    \n",
    "    ### Performing nms ###\n",
    "    final_boxes = []\n",
    "    pick = __nms(np.array(boxes), np.ones((len(boxes))), nms_threshold, 'Min')\n",
    "    for i in range(len(boxes)):\n",
    "        final_boxes.append(boxes[i])\n",
    "        \n",
    "    crops = []\n",
    "    if(visualize):\n",
    "        fig, ax = plt.subplots(1, len(conf_maps_viz) + 1, figsize=(5*(len(conf_maps_viz) + 1), 5))\n",
    "        for i in range(len(conf_maps_viz)):\n",
    "            current_size = (H // (scale_factor ** i),W // (scale_factor**i))\n",
    "            ax[i].imshow(conf_maps_viz[i])\n",
    "            ax[i].set_title(f'Stage {i+1} {current_size}')\n",
    "        \n",
    "    raw_img_copy = raw_img.copy()\n",
    "    for (x, y, w, h) in final_boxes:\n",
    "        cv2.rectangle(raw_img_copy, (x,y), (x+w, y+h), (0,0,255),2)\n",
    "        crops.append([(x,y,w,h), raw_img[y:y+h, x:x+w]])\n",
    "            \n",
    "    if(visualize):\n",
    "        plt.imshow(raw_img_copy)\n",
    "        plt.show()\n",
    "        \n",
    "    return final_boxes, crops\n",
    "        \n",
    "boxes, crops = get_bboxes_pnet(raw_img, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_img = cv2.imread('test/test9.jpg')\n",
    "raw_img = cv2.cvtColor(raw_img, cv2.COLOR_BGR2RGB)\n",
    "boxes, crops = get_bboxes_pnet(raw_img, visualize=True)\n",
    "\n",
    "raw_img_copy = raw_img.copy()\n",
    "boxes = []\n",
    "\n",
    "n_frames = 0\n",
    "start = time.time()\n",
    "for i in crops:\n",
    "    n_frames += 1\n",
    "    crop = i[1]\n",
    "    (x, y, w, h) = i[0]\n",
    "    H, W = crop.shape[:2]\n",
    "    img = cv2.resize(crop, (input_dim*4, input_dim*4))\n",
    "    img = (img - 127.5) / 127.5\n",
    "    \n",
    "    prediction = onet.predict(np.array([img]))\n",
    "    confidence = prediction[0][0][0][0]\n",
    "    \n",
    "    label = np.argmax(confidence)\n",
    "    confidence = confidence[np.argmax(confidence)]\n",
    "    \n",
    "    bbox = prediction[1][0][0][0]\n",
    "    if(confidence < 0.98 or label == 0):continue\n",
    "    \n",
    "    x_,y_,w,h = (bbox * np.array([W, H, W, H])).astype('int')\n",
    "    x+=x_\n",
    "    y+=y_\n",
    "    \n",
    "    boxes.append([x, y, w, h])\n",
    "end = time.time()\n",
    "print(f'FPS : {n_frames/(end - start)}')\n",
    "\n",
    "pick = __nms(np.array(boxes), np.ones((len(boxes))), 0.3, 'Max')\n",
    "for i in pick:\n",
    "    x, y, w, h = boxes[i]\n",
    "    raw_img_copy = cv2.rectangle(raw_img_copy, (x, y), (x+w, y+h), (0,255,0), 3)\n",
    "    raw_img_copy = cv2.putText(raw_img_copy, f'{confidence:.2f}', (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,0,0), 3)\n",
    "    \n",
    "plt.imshow(raw_img_copy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
